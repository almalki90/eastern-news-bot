#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ø¨ÙˆØª Ø£Ø®Ø¨Ø§Ø± Ø§Ù„ÙˆØ¸Ø§Ø¦Ù - Ù…ÙˆÙ‚Ø¹ Ø£ÙŠ ÙˆØ¸ÙŠÙØ© ÙÙ‚Ø·
ÙŠØ³ØªØ®Ø±Ø¬ Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ù…Ø¨Ø§Ø´Ø±Ø© Ù…Ù† ewdifh.com
ÙŠØ¹Ù…Ù„ ÙŠÙˆÙ…ÙŠØ§Ù‹ Ø§Ù„Ø³Ø§Ø¹Ø© 06:00 Ùˆ 18:00 (UTC)
"""

import requests
from bs4 import BeautifulSoup
import json
import os
import re
from datetime import datetime

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¨ÙˆØª
BOT_TOKEN = os.environ.get('BOT_TOKEN', '8281406621:AAGpJOnC1Ua1I4t49h8kWea-7pND8zTSBhg')
TELEGRAM_API = f'https://api.telegram.org/bot{BOT_TOKEN}'
SENT_NEWS_FILE = 'sent_jobs.json'

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…ÙˆÙ‚Ø¹ Ø£ÙŠ ÙˆØ¸ÙŠÙØ©
EWDIFH_URL = "https://www.ewdifh.com/category/all-jobs"
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"

# ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© Ù„Ù„Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø´Ø±Ù‚ÙŠØ©
EASTERN_KEYWORDS = [
    # Ø§Ù„Ù…Ù†Ø·Ù‚Ø© ÙˆØ§Ù„Ù…Ø¯Ù† Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
    'Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø´Ø±Ù‚ÙŠØ©', 'Ø§Ù„Ø´Ø±Ù‚ÙŠØ©', 'eastern province', 'eastern region',
    'Ø§Ù„Ø¯Ù…Ø§Ù…', 'dammam', 'Ø§Ù„Ø®Ø¨Ø±', 'khobar', 'al khobar', 'Ø§Ù„Ø¸Ù‡Ø±Ø§Ù†', 'dhahran',
    'Ø§Ù„Ø¬Ø¨ÙŠÙ„', 'jubail', 'Ø§Ù„Ø£Ø­Ø³Ø§Ø¡', 'al ahsa', 'ahsa', 'hofuf',
    'Ø§Ù„Ù‚Ø·ÙŠÙ', 'qatif', 'al qatif', 'Ø­ÙØ± Ø§Ù„Ø¨Ø§Ø·Ù†', 'hafr al batin',
    # Ø£Ø­ÙŠØ§Ø¡ ÙˆÙ…Ø¹Ø§Ù„Ù…
    'Ø§Ù„Ø±Ø§ÙƒØ©', 'Ø§Ù„ÙÙŠØµÙ„ÙŠØ©', 'Ø§Ù„Ø¹Ø²ÙŠØ²ÙŠØ©', 'Ø§Ù„Ù†Ø²Ù‡Ø©', 'Ø§Ù„Ø´Ø§Ø·Ø¦',
    'Ø£Ø±Ø§Ù…ÙƒÙˆ', 'aramco', 'saudi aramco',
    'Ø³Ø§Ø¨Ùƒ', 'sabic',
    'Ù…Ø·Ø§Ø± Ø§Ù„Ù…Ù„Ùƒ ÙÙ‡Ø¯', 'king fahd airport',
    'Ø¬Ø§Ù…Ø¹Ø© Ø§Ù„Ø¯Ù…Ø§Ù…', 'Ø¬Ø§Ù…Ø¹Ø© Ø§Ù„Ø¥Ù…Ø§Ù… Ø¹Ø¨Ø¯Ø§Ù„Ø±Ø­Ù…Ù†', 'iau',
    'Ø¬Ø§Ù…Ø¹Ø© Ø§Ù„Ù…Ù„Ùƒ ÙÙ‡Ø¯', 'kfupm',
    # Ù…Ø­Ø§ÙØ¸Ø§Øª
    'Ø±Ø£Ø³ ØªÙ†ÙˆØ±Ø©', 'ras tanura',
    'Ø§Ù„Ù†Ø¹ÙŠØ±ÙŠØ©', 'Ø§Ù„Ø®ÙØ¬ÙŠ', 'khafji',
    'Ø¨Ù‚ÙŠÙ‚', 'buqayq',
    'Ø§Ù„Ø¬Ø¨ÙŠÙ„ Ø§Ù„ØµÙ†Ø§Ø¹ÙŠØ©', 'jubail industrial'
]

def load_sent():
    """ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ù…Ø±Ø³Ù„Ø© Ø³Ø§Ø¨Ù‚Ø§Ù‹"""
    if os.path.exists(SENT_NEWS_FILE):
        try:
            with open(SENT_NEWS_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            return {}
    return {}

def save_sent(data):
    """Ø­ÙØ¸ Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ù…Ø±Ø³Ù„Ø©"""
    with open(SENT_NEWS_FILE, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def fetch_jobs_page(page=1):
    """Ø¬Ù„Ø¨ ØµÙØ­Ø© Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ù…Ù† Ù…ÙˆÙ‚Ø¹ Ø£ÙŠ ÙˆØ¸ÙŠÙØ©"""
    try:
        url = f"{EWDIFH_URL}?page={page}" if page > 1 else EWDIFH_URL
        headers = {
            'User-Agent': USER_AGENT,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ar,en-US;q=0.9,en;q=0.8',
            'Connection': 'keep-alive',
        }
        
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        return response.text
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„ØµÙØ­Ø© {page}: {e}")
        return None

def parse_jobs(html):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ù…Ù† HTML"""
    jobs = []
    
    try:
        soup = BeautifulSoup(html, 'html.parser')
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ÙˆØ¸Ø§Ø¦Ù
        job_links = soup.find_all('a', href=re.compile(r'https://www\.ewdifh\.com/jobs/\d+'))
        
        for link in job_links:
            try:
                job_url = link.get('href')
                
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
                title_elem = link.find('h3') or link.find('h2') or link
                title = title_elem.get_text(strip=True) if title_elem else ""
                
                # ØªØ¬Ù†Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø±
                if title and not any(j['link'] == job_url for j in jobs):
                    jobs.append({
                        'title': title,
                        'link': job_url,
                        'source': 'Ù…ÙˆÙ‚Ø¹ Ø£ÙŠ ÙˆØ¸ÙŠÙØ©',
                        'id': job_url,
                        'summary': '',
                        'published': datetime.now().isoformat()
                    })
            except:
                continue
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±
        unique_jobs = []
        seen_urls = set()
        for job in jobs:
            if job['link'] not in seen_urls:
                unique_jobs.append(job)
                seen_urls.add(job['link'])
        
        return unique_jobs
        
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ HTML: {e}")
        return []

def fetch_job_details(job_url):
    """Ø¬Ù„Ø¨ ØªÙØ§ØµÙŠÙ„ Ø§Ù„ÙˆØ¸ÙŠÙØ©"""
    try:
        headers = {'User-Agent': USER_AGENT}
        response = requests.get(job_url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
        content_div = soup.find('div', class_=['post-content', 'entry-content', 'content'])
        if content_div:
            content = content_div.get_text(strip=True)[:500]
            return content
        
        return ""
    except:
        return ""

def is_eastern_province(job):
    """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„ÙˆØ¸ÙŠÙØ© ÙÙŠ Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø´Ø±Ù‚ÙŠØ©"""
    text = f"{job.get('title', '')} {job.get('summary', '')}".lower()
    return any(k.lower() in text for k in EASTERN_KEYWORDS)

def get_chat_ids():
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ø±ÙØ§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ù…Ù† Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø© Ø£Ùˆ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©"""
    # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ IDs Ù…Ù† Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø©
    chat_ids_str = os.environ.get('CHAT_IDS', '')
    
    if chat_ids_str:
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ø£Ø±Ù‚Ø§Ù…
        try:
            return [int(id.strip()) for id in chat_ids_str.split(',') if id.strip()]
        except:
            pass
    
    # Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
    default_ids = [
        -1003882183490,  # Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        # -1001234567890,  # Ù…Ø¬Ù…ÙˆØ¹Ø© Dammam2030 (Ø³ÙŠØªÙ… ØªØ­Ø¯ÙŠØ«Ù‡ Ø¨Ø¹Ø¯ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ID)
    ]
    return default_ids

def send_message(chat_id, message):
    """Ø¥Ø±Ø³Ø§Ù„ Ø±Ø³Ø§Ù„Ø© Ø¥Ù„Ù‰ ØªÙ„ÙŠØ¬Ø±Ø§Ù…"""
    try:
        payload = {
            'chat_id': chat_id,
            'text': message,
            'parse_mode': 'Markdown',
            'disable_web_page_preview': True
        }
        response = requests.post(f'{TELEGRAM_API}/sendMessage', json=payload, timeout=10)
        return response.status_code == 200
    except:
        return False

def send_to_all_chats(message, chat_ids):
    """Ø¥Ø±Ø³Ø§Ù„ Ø±Ø³Ø§Ù„Ø© Ø¥Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª"""
    success_count = 0
    failed_chats = []
    
    for chat_id in chat_ids:
        if send_message(chat_id, message):
            success_count += 1
            print(f"  âœ… ØªÙ… Ø§Ù„Ø¥Ø±Ø³Ø§Ù„ Ø¥Ù„Ù‰ {chat_id}")
        else:
            failed_chats.append(chat_id)
            print(f"  âŒ ÙØ´Ù„ Ø§Ù„Ø¥Ø±Ø³Ø§Ù„ Ø¥Ù„Ù‰ {chat_id}")
    
    return success_count, failed_chats

def main():
    print(f"\nğŸ’¼ Ø¨ÙˆØª ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø´Ø±Ù‚ÙŠØ© - Ù…ÙˆÙ‚Ø¹ Ø£ÙŠ ÙˆØ¸ÙŠÙØ©")
    print(f"ğŸ• {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    sent = load_sent()
    chat_ids = get_chat_ids()
    
    print(f"\nğŸ“± Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‡Ø¯ÙØ©: {len(chat_ids)}")
    for chat_id in chat_ids:
        print(f"  â€¢ {chat_id}")
    
    # Ø¬Ù„Ø¨ Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ù…Ù† ØµÙØ­ØªÙŠÙ† (Ø­ÙˆØ§Ù„ÙŠ 40 ÙˆØ¸ÙŠÙØ©)
    all_jobs = []
    for page in range(1, 3):
        print(f"\nğŸ“„ Ø¬Ù„Ø¨ ØµÙØ­Ø© {page}...")
        html = fetch_jobs_page(page)
        
        if html:
            jobs = parse_jobs(html)
            all_jobs.extend(jobs)
            print(f"  âœ… Ø§Ø³ØªØ®Ø±Ø¬Øª {len(jobs)} ÙˆØ¸ÙŠÙØ©")
        else:
            print(f"  âš ï¸ ÙØ´Ù„ Ø¬Ù„Ø¨ Ø§Ù„ØµÙØ­Ø© {page}")
    
    print(f"\nğŸ“Š Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ÙˆØ¸Ø§Ø¦Ù: {len(all_jobs)}")
    
    # Ø¬Ù„Ø¨ ØªÙØ§ØµÙŠÙ„ Ø£ÙˆÙ„ 20 ÙˆØ¸ÙŠÙØ©
    print("\nğŸ“ Ø¬Ù„Ø¨ ØªÙØ§ØµÙŠÙ„ Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ù„Ù„ÙÙ„ØªØ±Ø©...")
    for i, job in enumerate(all_jobs[:20], 1):
        job['summary'] = fetch_job_details(job['link'])
        if i % 5 == 0:
            print(f"  â³ {i}/20...")
    
    # ÙÙ„ØªØ±Ø© Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø´Ø±Ù‚ÙŠØ©
    eastern_jobs = [j for j in all_jobs if is_eastern_province(j)]
    excluded = len(all_jobs) - len(eastern_jobs)
    
    print(f"\nâœ… ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø´Ø±Ù‚ÙŠØ©: {len(eastern_jobs)}")
    print(f"âŒ Ù…Ø³ØªØ¨Ø¹Ø¯ (Ø®Ø§Ø±Ø¬ Ø§Ù„Ù…Ù†Ø·Ù‚Ø©): {excluded}")
    
    # ÙÙ„ØªØ±Ø© Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
    new_jobs = [j for j in eastern_jobs if j['id'] not in sent]
    
    print(f"\nğŸ’¼ ÙˆØ¸Ø§Ø¦Ù Ø¬Ø¯ÙŠØ¯Ø©: {len(new_jobs)}")
    
    if new_jobs:
        # Ø¥Ø±Ø³Ø§Ù„ Ø£ÙˆÙ„ 6 ÙˆØ¸Ø§Ø¦Ù
        message = "ğŸ’¼ *ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø´Ø±Ù‚ÙŠØ© - Ø£ÙŠ ÙˆØ¸ÙŠÙØ©*\n" + "â”" * 30 + "\n\n"
        
        for job in new_jobs[:6]:
            message += f"â€¢ {job['title']}\n"
            message += f"  ğŸ”— [Ø§Ù„ØªÙØ§ØµÙŠÙ„]({job['link']})\n\n"
            sent[job['id']] = {
                'title': job['title'],
                'sent_at': datetime.now().isoformat()
            }
        
        # Ø¥Ø±Ø³Ø§Ù„ Ø¥Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª
        print(f"\nğŸ“¤ Ø¥Ø±Ø³Ø§Ù„ {len(new_jobs[:6])} ÙˆØ¸ÙŠÙØ© Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª...")
        success_count, failed_chats = send_to_all_chats(message, chat_ids)
        
        if success_count > 0:
            print(f"\nâœ… ØªÙ… Ø§Ù„Ø¥Ø±Ø³Ø§Ù„ Ø¨Ù†Ø¬Ø§Ø­ Ø¥Ù„Ù‰ {success_count}/{len(chat_ids)} Ù…Ø¬Ù…ÙˆØ¹Ø©")
            save_sent(sent)
        else:
            print(f"\nâŒ ÙØ´Ù„ Ø§Ù„Ø¥Ø±Ø³Ø§Ù„ Ø¥Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª")
        
        if failed_chats:
            print(f"âš ï¸ ÙØ´Ù„ Ø§Ù„Ø¥Ø±Ø³Ø§Ù„ Ø¥Ù„Ù‰: {failed_chats}")
    else:
        print("â„¹ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ ÙˆØ¸Ø§Ø¦Ù Ø¬Ø¯ÙŠØ¯Ø©")
    
    print("=" * 60)

if __name__ == '__main__':
    main()
