#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Web Scraper Ù„Ù…ÙˆÙ‚Ø¹ Ø£ÙŠ ÙˆØ¸ÙŠÙØ© (ewdifh.com)
Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø°ÙƒÙŠ Ù„Ù„ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ ÙÙ„ØªØ±Ø© Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø´Ø±Ù‚ÙŠØ©
"""

import requests
from bs4 import BeautifulSoup
import json
import re
from datetime import datetime, timedelta
from dateutil import parser as date_parser

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
EWDIFH_URL = "https://www.ewdifh.com/category/all-jobs"
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"

EASTERN_KEYWORDS = [
    # Ø§Ù„Ù…Ù†Ø·Ù‚Ø© ÙˆØ§Ù„Ù…Ø¯Ù† Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
    'Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø´Ø±Ù‚ÙŠØ©', 'Ø§Ù„Ø´Ø±Ù‚ÙŠØ©', 'eastern province', 'eastern region',
    'Ø§Ù„Ø¯Ù…Ø§Ù…', 'dammam', 'Ø§Ù„Ø®Ø¨Ø±', 'khobar', 'al khobar', 'Ø§Ù„Ø¸Ù‡Ø±Ø§Ù†', 'dhahran',
    'Ø§Ù„Ø¬Ø¨ÙŠÙ„', 'jubail', 'Ø§Ù„Ø£Ø­Ø³Ø§Ø¡', 'al ahsa', 'ahsa', 'hofuf',
    'Ø§Ù„Ù‚Ø·ÙŠÙ', 'qatif', 'al qatif', 'Ø­ÙØ± Ø§Ù„Ø¨Ø§Ø·Ù†', 'hafr al batin',
    # Ø£Ø­ÙŠØ§Ø¡ ÙˆÙ…Ø¹Ø§Ù„Ù…
    'Ø§Ù„Ø±Ø§ÙƒØ©', 'Ø§Ù„ÙÙŠØµÙ„ÙŠØ©', 'Ø§Ù„Ø¹Ø²ÙŠØ²ÙŠØ©', 'Ø§Ù„Ù†Ø²Ù‡Ø©', 'Ø§Ù„Ø´Ø§Ø·Ø¦',
    'Ø£Ø±Ø§Ù…ÙƒÙˆ', 'aramco', 'saudi aramco',
    'Ø³Ø§Ø¨Ùƒ', 'sabic',
    'Ù…Ø·Ø§Ø± Ø§Ù„Ù…Ù„Ùƒ ÙÙ‡Ø¯', 'king fahd airport',
    'Ø¬Ø§Ù…Ø¹Ø© Ø§Ù„Ø¯Ù…Ø§Ù…', 'Ø¬Ø§Ù…Ø¹Ø© Ø§Ù„Ø¥Ù…Ø§Ù… Ø¹Ø¨Ø¯Ø§Ù„Ø±Ø­Ù…Ù†', 'iau',
    'Ø¬Ø§Ù…Ø¹Ø© Ø§Ù„Ù…Ù„Ùƒ ÙÙ‡Ø¯', 'kfupm',
    # Ù…Ø­Ø§ÙØ¸Ø§Øª
    'Ø±Ø£Ø³ ØªÙ†ÙˆØ±Ø©', 'ras tanura',
    'Ø§Ù„Ù†Ø¹ÙŠØ±ÙŠØ©', 'Ø§Ù„Ø®ÙØ¬ÙŠ', 'khafji',
    'Ø¨Ù‚ÙŠÙ‚', 'buqayq',
    'Ø§Ù„Ø¬Ø¨ÙŠÙ„ Ø§Ù„ØµÙ†Ø§Ø¹ÙŠØ©', 'jubail industrial'
]

def fetch_jobs_page(page=1):
    """
    Ø¬Ù„Ø¨ ØµÙØ­Ø© Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ù…Ù† Ù…ÙˆÙ‚Ø¹ Ø£ÙŠ ÙˆØ¸ÙŠÙØ©
    """
    try:
        url = f"{EWDIFH_URL}?page={page}" if page > 1 else EWDIFH_URL
        headers = {
            'User-Agent': USER_AGENT,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ar,en-US;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
        }
        
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        return response.text
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„ØµÙØ­Ø© {page}: {e}")
        return None

def parse_jobs(html):
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ù…Ù† HTML
    """
    jobs = []
    
    try:
        soup = BeautifulSoup(html, 'html.parser')
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ÙˆØ¸Ø§Ø¦Ù
        job_links = soup.find_all('a', href=re.compile(r'https://www\.ewdifh\.com/jobs/\d+'))
        
        print(f"  ğŸ” ÙˆØ¬Ø¯Øª {len(job_links)} Ø±Ø§Ø¨Ø· ÙˆØ¸ÙŠÙØ©")
        
        for link in job_links:
            try:
                job_url = link.get('href')
                
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
                title_elem = link.find('h3') or link.find('h2') or link
                title = title_elem.get_text(strip=True) if title_elem else ""
                
                # ØªØ¬Ù†Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø± (Ù†ÙØ³ Ø§Ù„Ø±Ø§Ø¨Ø· Ù‚Ø¯ ÙŠØ¸Ù‡Ø± Ù…Ø±ØªÙŠÙ†)
                if not any(j['link'] == job_url for j in jobs):
                    jobs.append({
                        'title': title,
                        'link': job_url,
                        'source': 'Ù…ÙˆÙ‚Ø¹ Ø£ÙŠ ÙˆØ¸ÙŠÙØ©',
                        'id': job_url,
                        'published': datetime.now().isoformat()
                    })
            except Exception as e:
                continue
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø± Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø±Ø§Ø¨Ø·
        unique_jobs = []
        seen_urls = set()
        for job in jobs:
            if job['link'] not in seen_urls:
                unique_jobs.append(job)
                seen_urls.add(job['link'])
        
        return unique_jobs
        
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ HTML: {e}")
        return []

def fetch_job_details(job_url):
    """
    Ø¬Ù„Ø¨ ØªÙØ§ØµÙŠÙ„ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ù…Ù† ØµÙØ­ØªÙ‡Ø§ Ø§Ù„Ø®Ø§ØµØ©
    """
    try:
        headers = {'User-Agent': USER_AGENT}
        response = requests.get(job_url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
        content_div = soup.find('div', class_=['post-content', 'entry-content', 'content'])
        if content_div:
            # Ø£Ø®Ø° Ø£ÙˆÙ„ 500 Ø­Ø±Ù Ù…Ù† Ø§Ù„Ù…Ø­ØªÙˆÙ‰
            content = content_div.get_text(strip=True)[:500]
            return content
        
        return ""
    except:
        return ""

def is_eastern_province(job):
    """
    Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„ÙˆØ¸ÙŠÙØ© ÙÙŠ Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø´Ø±Ù‚ÙŠØ©
    """
    text = f"{job.get('title', '')} {job.get('summary', '')}".lower()
    return any(k.lower() in text for k in EASTERN_KEYWORDS)

def scrape_ewdifh_jobs(max_pages=2):
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ù…Ù† Ù…ÙˆÙ‚Ø¹ Ø£ÙŠ ÙˆØ¸ÙŠÙØ©
    """
    print("\nğŸ” Ø¨Ø¯Ø¡ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ù…Ù† Ù…ÙˆÙ‚Ø¹ Ø£ÙŠ ÙˆØ¸ÙŠÙØ©...")
    print("=" * 60)
    
    all_jobs = []
    
    for page in range(1, max_pages + 1):
        print(f"\nğŸ“„ ØµÙØ­Ø© {page}...")
        html = fetch_jobs_page(page)
        
        if not html:
            print(f"  âš ï¸ ÙØ´Ù„ Ø¬Ù„Ø¨ Ø§Ù„ØµÙØ­Ø© {page}")
            continue
        
        jobs = parse_jobs(html)
        print(f"  âœ… Ø§Ø³ØªØ®Ø±Ø¬Øª {len(jobs)} ÙˆØ¸ÙŠÙØ©")
        
        all_jobs.extend(jobs)
    
    print(f"\nğŸ“Š Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©: {len(all_jobs)}")
    
    # Ø¬Ù„Ø¨ ØªÙØ§ØµÙŠÙ„ Ø£ÙˆÙ„ 20 ÙˆØ¸ÙŠÙØ© Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„ÙÙ„ØªØ±Ø©
    print("\nğŸ“ Ø¬Ù„Ø¨ ØªÙØ§ØµÙŠÙ„ Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„ÙÙ„ØªØ±Ø©...")
    for i, job in enumerate(all_jobs[:20], 1):
        print(f"  {i}/20: {job['title'][:50]}...")
        job['summary'] = fetch_job_details(job['link'])
    
    # ÙÙ„ØªØ±Ø© Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø´Ø±Ù‚ÙŠØ©
    eastern_jobs = [j for j in all_jobs if is_eastern_province(j)]
    excluded_jobs = [j for j in all_jobs if not is_eastern_province(j)]
    
    print(f"\nâœ… ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø´Ø±Ù‚ÙŠØ©: {len(eastern_jobs)}")
    print(f"âŒ Ù…Ø³ØªØ¨Ø¹Ø¯ (Ø®Ø§Ø±Ø¬ Ø§Ù„Ù…Ù†Ø·Ù‚Ø©): {len(excluded_jobs)}")
    
    if excluded_jobs and len(excluded_jobs) <= 5:
        print("\nâš ï¸ Ø£Ù…Ø«Ù„Ø© Ø¹Ù„Ù‰ Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ù…Ø³ØªØ¨Ø¹Ø¯Ø©:")
        for job in excluded_jobs[:3]:
            print(f"  â€¢ {job['title'][:80]}")
    
    print("=" * 60)
    
    return eastern_jobs

def save_jobs_json(jobs, filename='scraped_jobs.json'):
    """
    Ø­ÙØ¸ Ø§Ù„ÙˆØ¸Ø§Ø¦Ù ÙÙŠ Ù…Ù„Ù JSON
    """
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(jobs, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ {len(jobs)} ÙˆØ¸ÙŠÙØ© ÙÙŠ {filename}")

if __name__ == '__main__':
    # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø³ÙƒØ±ÙŠØ¨Øª
    jobs = scrape_ewdifh_jobs(max_pages=2)
    
    if jobs:
        save_jobs_json(jobs)
        
        print("\nğŸ“‹ Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©:")
        for i, job in enumerate(jobs[:5], 1):
            print(f"\n{i}. {job['title']}")
            print(f"   ğŸ”— {job['link']}")
    else:
        print("\nâš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£ÙŠ ÙˆØ¸Ø§Ø¦Ù")
