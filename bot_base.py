#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ø¨ÙˆØª ØªÙ„ÙŠØ¬Ø±Ø§Ù… Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø´Ø±Ù‚ÙŠØ© - Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©
ÙŠØ¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù† RSS feeds ÙˆÙŠØ±Ø³Ù„Ù‡Ø§ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ù„Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª
"""

import feedparser
import requests
import json
import os
import time
import re
from datetime import datetime, timedelta
from typing import List, Dict
from dateutil import parser as date_parser

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¨ÙˆØª
BOT_TOKEN = os.environ.get('BOT_TOKEN', '8281406621:AAGpJOnC1Ua1I4t49h8kWea-7pND8zTSBhg')
TELEGRAM_API = f'https://api.telegram.org/bot{BOT_TOKEN}'

# Ù…ØµØ§Ø¯Ø± RSS Ù„Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø¹Ø§Ù…Ø©
GENERAL_NEWS_FEEDS = [
    # Google News - Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø´Ø±Ù‚ÙŠØ© (Ø¨Ø­Ø« Ù…Ø®ØµØµ)
    {
        'name': 'Google News - Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø´Ø±Ù‚ÙŠØ©',
        'url': 'https://news.google.com/rss/search?q=Ø§Ù„Ù…Ù†Ø·Ù‚Ø©+Ø§Ù„Ø´Ø±Ù‚ÙŠØ©+OR+Ø§Ù„Ø¯Ù…Ø§Ù…+OR+Ø§Ù„Ø®Ø¨Ø±+OR+Ø§Ù„Ø¸Ù‡Ø±Ø§Ù†+when:7d&hl=ar&gl=SA&ceid=SA:ar',
        'enabled': True
    },
    {
        'name': 'Google News - Ø§Ù„Ø¯Ù…Ø§Ù… Ø§Ù„Ø®Ø¨Ø±',
        'url': 'https://news.google.com/rss/search?q=Ø§Ù„Ø¯Ù…Ø§Ù…+OR+Ø§Ù„Ø®Ø¨Ø±+OR+Ø§Ù„Ù‚Ø·ÙŠÙ+when:7d&hl=ar&gl=SA&ceid=SA:ar',
        'enabled': True
    },
    {
        'name': 'Google News - Ø§Ù„Ø£Ø­Ø³Ø§Ø¡ Ø§Ù„Ø¬Ø¨ÙŠÙ„',
        'url': 'https://news.google.com/rss/search?q=Ø§Ù„Ø£Ø­Ø³Ø§Ø¡+OR+Ø§Ù„Ø¬Ø¨ÙŠÙ„+OR+Ø­ÙØ±+Ø§Ù„Ø¨Ø§Ø·Ù†+when:7d&hl=ar&gl=SA&ceid=SA:ar',
        'enabled': True
    },
    # Ù…ØµØ§Ø¯Ø± Ø¹Ø±Ø¨ÙŠØ© Ø¹Ø§Ù…Ø© (Ù„Ù„ÙÙ„ØªØ±Ø©)
    {
        'name': 'Ø¹Ø±Ø¨ Ù†ÙŠÙˆØ² - Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©',
        'url': 'https://www.arabnews.com/rss/saudi-arabia',
        'enabled': True
    },
    {
        'name': 'Ø§Ù„Ø´Ø±Ù‚ Ø§Ù„Ø£ÙˆØ³Ø·',
        'url': 'https://aawsat.com/feed',
        'enabled': True
    }
]

# Ù…ØµØ§Ø¯Ø± RSS Ù„Ù„ÙˆØ¸Ø§Ø¦Ù - Ù…Ø®ØµØµØ© ÙˆÙ…Ù†ÙØµÙ„Ø©
JOBS_NEWS_FEEDS = [
    {
        'name': 'Google News - ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ø´Ø±Ù‚ÙŠØ©',
        'url': 'https://news.google.com/rss/search?q=ÙˆØ¸Ø§Ø¦Ù+OR+ØªÙˆØ¸ÙŠÙ+OR+ÙØ±Øµ+Ø¹Ù…Ù„+(Ø§Ù„Ø¯Ù…Ø§Ù…+OR+Ø§Ù„Ø®Ø¨Ø±+OR+Ø§Ù„Ø¬Ø¨ÙŠÙ„+OR+Ø§Ù„Ø£Ø­Ø³Ø§Ø¡+OR+Ø§Ù„Ø´Ø±Ù‚ÙŠØ©)+when:3d&hl=ar&gl=SA&ceid=SA:ar',
        'enabled': True
    },
    {
        'name': 'Google News - ÙˆØ¸Ø§Ø¦Ù Dammam',
        'url': 'https://news.google.com/rss/search?q=jobs+hiring+employment+(Dammam+OR+Khobar+OR+Dhahran+OR+Eastern)+when:3d&hl=en&gl=SA&ceid=SA:en',
        'enabled': True
    }
]

# Ù…ØµØ§Ø¯Ø± RSS Ù„Ù„Ø·Ù‚Ø³ - Ù…Ø®ØµØµØ© ÙˆÙ…Ù†ÙØµÙ„Ø©
WEATHER_NEWS_FEEDS = [
    {
        'name': 'Google News - Ø·Ù‚Ø³ Ø§Ù„Ø´Ø±Ù‚ÙŠØ©',
        'url': 'https://news.google.com/rss/search?q=Ø·Ù‚Ø³+OR+Ø­Ø§Ù„Ø©+Ø§Ù„Ø¬Ùˆ+OR+Ø§Ù„Ø£Ø±ØµØ§Ø¯+(Ø§Ù„Ø¯Ù…Ø§Ù…+OR+Ø§Ù„Ø®Ø¨Ø±+OR+Ø§Ù„Ù…Ù†Ø·Ù‚Ø©+Ø§Ù„Ø´Ø±Ù‚ÙŠØ©)+when:1d&hl=ar&gl=SA&ceid=SA:ar',
        'enabled': True
    },
    {
        'name': 'Google News - Ø·Ù‚Ø³ Ø§Ù„Ø¹Ø±Ø¨',
        'url': 'https://news.google.com/rss/search?q=site:arabiaweather.com+(Ø§Ù„Ø¯Ù…Ø§Ù…+OR+Ø§Ù„Ø®Ø¨Ø±+OR+Ø§Ù„Ø´Ø±Ù‚ÙŠØ©)+when:1d&hl=ar&gl=SA&ceid=SA:ar',
        'enabled': True
    }
]

# Ù…Ù„Ù Ù„Ø­ÙØ¸ Ø¢Ø®Ø± Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ù…Ø±Ø³Ù„Ø©
SENT_NEWS_FILE = 'sent_news.json'

# ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© Ù„Ù„Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø´Ø±Ù‚ÙŠØ© - ÙŠØ¬Ø¨ Ø£Ù† ÙŠØ­ØªÙˆÙŠ Ø§Ù„Ø®Ø¨Ø± Ø¹Ù„Ù‰ ÙˆØ§Ø­Ø¯Ø© Ù…Ù†Ù‡Ø§ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„
EASTERN_PROVINCE_KEYWORDS = [
    # Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø´Ø±Ù‚ÙŠØ©
    'Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø´Ø±Ù‚ÙŠØ©', 'Ø§Ù„Ø´Ø±Ù‚ÙŠØ©',
    # Ø§Ù„Ù…Ø¯Ù† Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
    'Ø§Ù„Ø¯Ù…Ø§Ù…', 'dammam',
    'Ø§Ù„Ø®Ø¨Ø±', 'khobar', 'al khobar',
    'Ø§Ù„Ø¸Ù‡Ø±Ø§Ù†', 'dhahran',
    'Ø§Ù„Ø¬Ø¨ÙŠÙ„', 'jubail',
    'Ø§Ù„Ø£Ø­Ø³Ø§Ø¡', 'Ø§Ù„Ø§Ø­Ø³Ø§Ø¡', 'al-ahsa', 'al ahsa', 'ahsa',
    'Ø§Ù„Ù‡ÙÙˆÙ', 'hofuf',
    'Ø­ÙØ± Ø§Ù„Ø¨Ø§Ø·Ù†', 'hafr al-batin', 'hafar albatin',
    'Ø§Ù„Ù‚Ø·ÙŠÙ', 'qatif',
    'Ø§Ù„Ù†Ø¹ÙŠØ±ÙŠØ©', 'nairiyah',
    'Ø±Ø£Ø³ Ø§Ù„Ø®ÙŠØ±', 'ras al khair',
    'Ø§Ù„Ø®ÙØ¬ÙŠ', 'khafji',
    # Ù…Ø¹Ø§Ù„Ù… Ù…Ø´Ù‡ÙˆØ±Ø©
    'ÙƒÙˆØ±Ù†ÙŠØ´ Ø§Ù„Ø¯Ù…Ø§Ù…', 'ÙƒÙˆØ±Ù†ÙŠØ´ Ø§Ù„Ø®Ø¨Ø±',
    'Ø¬Ø§Ù…Ø¹Ø© Ø§Ù„Ø¯Ù…Ø§Ù…', 'Ø¬Ø§Ù…Ø¹Ø© Ø§Ù„Ù…Ù„Ùƒ ÙÙ‡Ø¯ Ù„Ù„Ø¨ØªØ±ÙˆÙ„',
    'Ø£Ø±Ø§Ù…ÙƒÙˆ', 'aramco',
    'Ø§Ù„Ù…Ù„Ùƒ ÙÙ‡Ø¯', 'king fahd',
    # Ø£Ø­ÙŠØ§Ø¡ ÙˆÙ…Ù†Ø§Ø·Ù‚
    'Ø§Ù„Ø±Ø§ÙƒØ©', 'Ø§Ù„Ø¹Ø²ÙŠØ²ÙŠØ©', 'Ø§Ù„ÙÙŠØµÙ„ÙŠØ©', 'Ø§Ù„Ø´Ø§Ø·Ø¦'
]


def load_sent_news() -> Dict:
    """ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ù…Ø±Ø³Ù„Ø© Ø³Ø§Ø¨Ù‚Ø§Ù‹"""
    if os.path.exists(SENT_NEWS_FILE):
        try:
            with open(SENT_NEWS_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            return {}
    return {}


def save_sent_news(sent_news: Dict):
    """Ø­ÙØ¸ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ù…Ø±Ø³Ù„Ø©"""
    with open(SENT_NEWS_FILE, 'w', encoding='utf-8') as f:
        json.dump(sent_news, f, ensure_ascii=False, indent=2)


def get_bot_chats() -> List[int]:
    """
    Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª/Ø§Ù„Ù‚Ù†ÙˆØ§Øª Ø§Ù„ØªÙŠ Ø§Ù„Ø¨ÙˆØª ÙÙŠÙ‡Ø§ ÙƒÙ…Ø´Ø±Ù
    """
    try:
        # Ø¬Ù„Ø¨ Ø§Ù„ØªØ­Ø¯ÙŠØ«Ø§Øª Ø§Ù„Ø£Ø®ÙŠØ±Ø©
        response = requests.get(f'{TELEGRAM_API}/getUpdates', timeout=10)
        updates = response.json()
        
        chat_ids = set()
        if updates.get('ok'):
            for update in updates.get('result', []):
                # Ø¬Ù„Ø¨ chat_id Ù…Ù† Ø§Ù„Ø±Ø³Ø§Ø¦Ù„
                if 'message' in update:
                    chat_id = update['message']['chat']['id']
                    chat_type = update['message']['chat']['type']
                    # ÙÙ‚Ø· Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª ÙˆØ§Ù„Ù‚Ù†ÙˆØ§Øª (Ù„ÙŠØ³ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ø®Ø§ØµØ©)
                    if chat_type in ['group', 'supergroup', 'channel']:
                        chat_ids.add(chat_id)
                        
        return list(chat_ids)
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª: {e}")
        return []


def is_protocol_news(news_item: Dict) -> bool:
    """
    Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„Ø®Ø¨Ø± Ø¨Ø±ÙˆØªÙˆÙƒÙˆÙ„ÙŠ (Ø§Ø³ØªÙ‚Ø¨Ø§Ù„ØŒ Ø²ÙŠØ§Ø±Ø§ØªØŒ ØªÙ‡Ù†Ø¦Ø©...)
    Ù†Ø±ÙŠØ¯ Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ Ù‡Ø°Ù‡ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±
    """
    title = news_item.get('title', '').lower()
    summary = news_item.get('summary', '').lower()
    full_text = f"{title} {summary}"
    
    # ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© Ù„Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø¨Ø±ÙˆØªÙˆÙƒÙˆÙ„ÙŠØ©
    protocol_keywords = [
        'Ø§Ø³ØªÙ‚Ø¨Ù„', 'ÙŠØ³ØªÙ‚Ø¨Ù„', 'Ø§Ø³ØªÙ‚Ø¨Ø§Ù„', 'ÙˆØ¯Ø¹', 'ÙŠÙˆØ¯Ø¹', 'ÙˆØ¯Ø§Ø¹',
        'Ø±Ø¹Ù‰', 'ÙŠØ±Ø¹Ù‰', 'Ø±Ø¹Ø§ÙŠØ©', 'Ø§ÙØªØªØ­ Ø­ÙÙ„', 'Ø­Ø¶Ø± Ø­ÙÙ„',
        'ÙƒØ±Ù…', 'ÙŠÙƒØ±Ù…', 'ØªÙƒØ±ÙŠÙ…', 'Ù‡Ù†Ø£', 'ÙŠÙ‡Ù†Ø¦', 'ØªÙ‡Ù†Ø¦Ø©',
        'Ø§Ù„ØªÙ‚Ù‰', 'ÙŠÙ„ØªÙ‚ÙŠ', 'Ù„Ù‚Ø§Ø¡', 'Ø²Ø§Ø±', 'ÙŠØ²ÙˆØ±', 'Ø²ÙŠØ§Ø±Ø©',
        'Ø§Ø·Ù„Ø¹ Ø¹Ù„Ù‰', 'ÙŠØ·Ù„Ø¹ Ø¹Ù„Ù‰', 'ØªÙÙ‚Ø¯', 'ÙŠØªÙÙ‚Ø¯'
    ]
    
    # Ø¥Ø°Ø§ ÙˆÙØ¬Ø¯Øª Ø£ÙŠ ÙƒÙ„Ù…Ø© Ø¨Ø±ÙˆØªÙˆÙƒÙˆÙ„ÙŠØ©
    for keyword in protocol_keywords:
        if keyword in full_text:
            return True
    
    return False


def is_valuable_news(news_item: Dict) -> bool:
    """
    Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„Ø®Ø¨Ø± Ù‚ÙŠÙ‘Ù… (ÙˆØ¸Ø§Ø¦ÙØŒ Ù…Ø´Ø§Ø±ÙŠØ¹ØŒ ØªØ±Ø³ÙŠØ§ØªØŒ Ù…Ù†Ø§Ù‚ØµØ§Øª...)
    """
    title = news_item.get('title', '').lower()
    summary = news_item.get('summary', '').lower()
    full_text = f"{title} {summary}"
    
    # ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© Ù„Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ù…Ù‡Ù…Ø©
    valuable_keywords = [
        # ÙˆØ¸Ø§Ø¦Ù
        'ÙˆØ¸ÙŠÙØ©', 'ÙˆØ¸Ø§Ø¦Ù', 'ØªÙˆØ¸ÙŠÙ', 'ØªØ¹ÙŠÙŠÙ†', 'ØªØ¹ÙŠÙŠÙ†Ø§Øª', 'ÙØ±Øµ Ø¹Ù…Ù„',
        'Ù…Ø³Ø§Ø¨Ù‚Ø© ÙˆØ¸ÙŠÙÙŠØ©', 'Ø¥Ø¹Ù„Ø§Ù† ÙˆØ¸ÙŠÙÙŠ', 'Ø±ÙˆØ§ØªØ¨', 'ØªÙˆØ¸ÙŠÙ', 'Ù…Ù‚Ø§Ø¨Ù„Ø©',
        'ØªÙ‚Ø¯ÙŠÙ… Ø·Ù„Ø¨', 'Ø³Ø¬Ù„ Ø§Ù„Ø¢Ù†', 'Ø§Ù„ØªÙ‚Ø¯ÙŠÙ…', 'Ø´ÙˆØ§ØºØ±',
        # Ù…Ø´Ø§Ø±ÙŠØ¹
        'Ù…Ø´Ø±ÙˆØ¹', 'Ù…Ø´Ø§Ø±ÙŠØ¹', 'ØªÙ†ÙÙŠØ° Ù…Ø´Ø±ÙˆØ¹', 'Ø¥Ø·Ù„Ø§Ù‚ Ù…Ø´Ø±ÙˆØ¹', 'Ù…Ø´Ø±ÙˆØ¹ ØªØ·ÙˆÙŠØ±',
        'Ø¨Ù†Ø§Ø¡', 'Ø¥Ù†Ø´Ø§Ø¡', 'ØªØ´ÙŠÙŠØ¯', 'ØªØ·ÙˆÙŠØ±', 'ØªÙˆØ³Ø¹Ø©',
        # ØªØ±Ø³ÙŠØ§Øª ÙˆÙ…Ù†Ø§Ù‚ØµØ§Øª
        'ØªØ±Ø³ÙŠØ©', 'ØªØ±Ø³ÙŠØ§Øª', 'Ù…Ù†Ø§Ù‚ØµØ©', 'Ù…Ù†Ø§Ù‚ØµØ§Øª', 'Ø¹Ù‚Ø¯', 'Ø¹Ù‚ÙˆØ¯',
        'Ù…Ø´ØªØ±ÙŠØ§Øª', 'Ø·Ø±Ø­', 'Ù…Ø²Ø§ÙŠØ¯Ø©', 'Ù…Ù†Ø§ÙØ³Ø©',
        # Ø§Ø³ØªØ«Ù…Ø§Ø± ÙˆØ§Ù‚ØªØµØ§Ø¯
        'Ø§Ø³ØªØ«Ù…Ø§Ø±', 'Ø§Ø³ØªØ«Ù…Ø§Ø±Ø§Øª', 'Ù…Ù„ÙŠØ§Ø±', 'Ù…Ù„ÙŠÙˆÙ†', 'Ø±ÙŠØ§Ù„',
        'Ø§Ù‚ØªØµØ§Ø¯', 'Ø§Ù‚ØªØµØ§Ø¯ÙŠ', 'ØªØ¬Ø§Ø±ÙŠ', 'ØµÙ†Ø§Ø¹ÙŠ',
        # Ø®Ø¯Ù…Ø§Øª Ø¹Ø§Ù…Ø©
        'Ø®Ø¯Ù…Ø©', 'Ø®Ø¯Ù…Ø§Øª', 'ØªØ´ØºÙŠÙ„', 'ØµÙŠØ§Ù†Ø©', 'Ù†Ø¸Ø§ÙØ©', 'Ø£Ù…Ù†',
        'Ù†Ù‚Ù„', 'Ø·Ø±Ù‚', 'Ø¬Ø³Ø±', 'ÙƒÙ‡Ø±Ø¨Ø§Ø¡', 'Ù…ÙŠØ§Ù‡', 'ØµØ±Ù ØµØ­ÙŠ',
        # ØªØ¹Ù„ÙŠÙ… ÙˆØµØ­Ø©
        'Ù…Ø¯Ø±Ø³Ø©', 'Ù…Ø³ØªØ´ÙÙ‰', 'Ù…Ø±ÙƒØ² ØµØ­ÙŠ', 'Ø¬Ø§Ù…Ø¹Ø©', 'Ù…Ø¹Ù‡Ø¯',
        'ØªØ¹Ù„ÙŠÙ…', 'ØµØ­Ø©', 'Ø·Ø¨ÙŠ', 'Ø¯Ø±Ø§Ø³ÙŠ',
        # Ø¹Ù‚Ø§Ø±Ø§Øª
        'Ø£Ø±Ø§Ø¶ÙŠ', 'Ø¹Ù‚Ø§Ø±', 'Ø¹Ù‚Ø§Ø±ÙŠ', 'Ø³ÙƒÙ†ÙŠ', 'Ø¥Ø³ÙƒØ§Ù†',
        # Ø·Ù‚Ø³ (Ù…Ù‡Ù… Ù„Ù„Ù…Ù†Ø·Ù‚Ø©)
        'Ø·Ù‚Ø³', 'Ø£Ù…Ø·Ø§Ø±', 'Ø­Ø±Ø§Ø±Ø©', 'Ø¯Ø±Ø¬Ø§Øª Ø§Ù„Ø­Ø±Ø§Ø±Ø©', 'Ø£Ø±ØµØ§Ø¯', 
        'Ø¶Ø¨Ø§Ø¨', 'ØºØ¨Ø§Ø±', 'Ø±ÙŠØ§Ø­', 'Ø£ØªØ±Ø¨Ø©', 'Ù…Ø«Ø§Ø±Ø©', 'Ø¹Ø§ØµÙØ©',
        'Ù…Ù†Ø®ÙØ¶ Ø¬ÙˆÙŠ', 'ØªÙ‚Ù„Ø¨Ø§Øª Ø¬ÙˆÙŠØ©', 'Ù…ÙˆØ¬Ø©', 'Ø§Ù„Ø·Ù‚Ø³ Ø§Ù„ÙŠÙˆÙ…'
    ]
    
    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£ÙŠ ÙƒÙ„Ù…Ø© Ù…ÙØªØ§Ø­ÙŠØ© Ù‚ÙŠÙ‘Ù…Ø©
    for keyword in valuable_keywords:
        if keyword in full_text:
            return True
    
    return False


def is_jobs_news(news_item: Dict) -> bool:
    """
    Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„Ø®Ø¨Ø± Ù…ØªØ¹Ù„Ù‚ Ø¨Ø§Ù„ÙˆØ¸Ø§Ø¦Ù
    """
    title = news_item.get('title', '').lower()
    summary = news_item.get('summary', '').lower()
    full_text = f"{title} {summary}"
    
    jobs_keywords = [
        'ÙˆØ¸ÙŠÙØ©', 'ÙˆØ¸Ø§Ø¦Ù', 'ØªÙˆØ¸ÙŠÙ', 'ØªØ¹ÙŠÙŠÙ†', 'ØªØ¹ÙŠÙŠÙ†Ø§Øª', 'ÙØ±Øµ Ø¹Ù…Ù„',
        'Ù…Ø³Ø§Ø¨Ù‚Ø© ÙˆØ¸ÙŠÙÙŠØ©', 'Ø¥Ø¹Ù„Ø§Ù† ÙˆØ¸ÙŠÙÙŠ', 'Ø±ÙˆØ§ØªØ¨', 'Ù…Ù‚Ø§Ø¨Ù„Ø©',
        'ØªÙ‚Ø¯ÙŠÙ… Ø·Ù„Ø¨', 'Ø³Ø¬Ù„ Ø§Ù„Ø¢Ù†', 'Ø§Ù„ØªÙ‚Ø¯ÙŠÙ…', 'Ø´ÙˆØ§ØºØ±',
        'job', 'jobs', 'hiring', 'employment', 'career', 'vacancies'
    ]
    
    for keyword in jobs_keywords:
        if keyword in full_text:
            return True
    return False


def is_weather_news(news_item: Dict) -> bool:
    """
    Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„Ø®Ø¨Ø± Ù…ØªØ¹Ù„Ù‚ Ø¨Ø§Ù„Ø·Ù‚Ø³
    """
    title = news_item.get('title', '').lower()
    summary = news_item.get('summary', '').lower()
    full_text = f"{title} {summary}"
    
    weather_keywords = [
        'Ø·Ù‚Ø³', 'Ø£Ù…Ø·Ø§Ø±', 'Ø­Ø±Ø§Ø±Ø©', 'Ø¯Ø±Ø¬Ø§Øª Ø§Ù„Ø­Ø±Ø§Ø±Ø©', 'Ø£Ø±ØµØ§Ø¯', 'Ø§Ù„Ù…Ø±ÙƒØ² Ø§Ù„ÙˆØ·Ù†ÙŠ Ù„Ù„Ø£Ø±ØµØ§Ø¯',
        'Ø¶Ø¨Ø§Ø¨', 'ØºØ¨Ø§Ø±', 'Ø±ÙŠØ§Ø­', 'Ø£ØªØ±Ø¨Ø©', 'Ù…Ø«Ø§Ø±Ø©', 'Ø¹Ø§ØµÙØ©', 'Ø±Ø¹Ø¯ÙŠØ©',
        'Ù…Ù†Ø®ÙØ¶ Ø¬ÙˆÙŠ', 'ØªÙ‚Ù„Ø¨Ø§Øª Ø¬ÙˆÙŠØ©', 'Ù…ÙˆØ¬Ø©', 'Ø§Ù„Ø·Ù‚Ø³ Ø§Ù„ÙŠÙˆÙ…', 'Ø­Ø§Ù„Ø© Ø§Ù„Ø¬Ùˆ',
        'Ø³Ø­Ø¨', 'Ù…Ù…Ø·Ø±Ø©', 'Ø¨Ø§Ø±Ø¯Ø©', 'Ø­Ø§Ø±Ø©', 'Ø±Ø·ÙˆØ¨Ø©', 'Ø¥Ù†Ø°Ø§Ø±', 'ØªØ­Ø°ÙŠØ±',
        'weather', 'temperature', 'rain', 'forecast', 'storm', 'wind'
    ]
    
    for keyword in weather_keywords:
        if keyword in full_text:
            return True
    return False


def is_recent_news(news_item: Dict, max_days: int = 2) -> bool:
    """
    Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„Ø®Ø¨Ø± Ø­Ø¯ÙŠØ« (Ø®Ù„Ø§Ù„ Ø¢Ø®Ø± ÙŠÙˆÙ…ÙŠÙ†)
    ÙŠØ³ØªØ¨Ø¹Ø¯ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© Ø®Ø§ØµØ© Ø§Ù„ÙˆØ¸Ø§Ø¦Ù ÙˆØ£Ø®Ø¨Ø§Ø± Ø§Ù„Ø·Ù‚Ø³
    """
    published_date = news_item.get('published', '')
    
    if not published_date:
        # Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù‡Ù†Ø§Ùƒ ØªØ§Ø±ÙŠØ®ØŒ Ù†Ù‚Ø¨Ù„ Ø§Ù„Ø®Ø¨Ø± (Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ø­Ø¯ÙŠØ«)
        return True
    
    try:
        # Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªØ§Ø±ÙŠØ®
        news_date = date_parser.parse(published_date)
        
        # Ø¥Ø²Ø§Ù„Ø© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª timezone Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø©
        if news_date.tzinfo:
            news_date = news_date.replace(tzinfo=None)
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„ØªØ§Ø±ÙŠØ® Ø§Ù„Ø­Ø§Ù„ÙŠ
        now = datetime.now()
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„ÙØ±Ù‚ Ø¨Ø§Ù„Ø£ÙŠØ§Ù…
        age_days = (now - news_date).days
        
        # Ù‚Ø¨ÙˆÙ„ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø­Ø¯ÙŠØ«Ø© ÙÙ‚Ø· (Ø®Ù„Ø§Ù„ Ø¢Ø®Ø± ÙŠÙˆÙ…ÙŠÙ†)
        if age_days <= max_days:
            return True
        else:
            print(f"   â° ØªÙ… Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ Ø®Ø¨Ø± Ù‚Ø¯ÙŠÙ… ({age_days} ÙŠÙˆÙ…): {news_item.get('title', '')[:50]}...")
            return False
            
    except Exception as e:
        # Ø¥Ø°Ø§ ÙØ´Ù„ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ§Ø±ÙŠØ®ØŒ Ù†Ù‚Ø¨Ù„ Ø§Ù„Ø®Ø¨Ø±
        return True


def is_eastern_province_news(news_item: Dict) -> bool:
    """
    Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„Ø®Ø¨Ø± ÙŠØªØ¹Ù„Ù‚ Ø¨Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø´Ø±Ù‚ÙŠØ©
    ÙŠØ¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© ÙÙŠ Ø§Ù„Ø¹Ù†ÙˆØ§Ù† ÙˆØ§Ù„Ù…Ù„Ø®Øµ
    """
    title = news_item.get('title', '').lower()
    summary = news_item.get('summary', '').lower()
    
    # Ø¯Ù…Ø¬ Ø§Ù„Ø¹Ù†ÙˆØ§Ù† ÙˆØ§Ù„Ù…Ù„Ø®Øµ Ù„Ù„Ø¨Ø­Ø«
    full_text = f"{title} {summary}"
    
    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£ÙŠ ÙƒÙ„Ù…Ø© Ù…ÙØªØ§Ø­ÙŠØ©
    for keyword in EASTERN_PROVINCE_KEYWORDS:
        if keyword.lower() in full_text:
            return True
    
    return False


def fetch_rss_news(feed_url: str, feed_name: str, max_items: int = 20) -> List[Dict]:
    """Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù† RSS feed"""
    try:
        print(f"ğŸ“¡ Ø¬Ù„Ø¨ Ø£Ø®Ø¨Ø§Ø± Ù…Ù†: {feed_name}")
        feed = feedparser.parse(feed_url)
        
        news_items = []
        for entry in feed.entries[:max_items]:
            news_item = {
                'title': entry.get('title', 'Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†'),
                'link': entry.get('link', ''),
                'summary': entry.get('summary', entry.get('description', '')),
                'published': entry.get('published', ''),
                'source': feed_name,
                'id': entry.get('id', entry.get('link', ''))
            }
            news_items.append(news_item)
        
        print(f"âœ… ØªÙ… Ø¬Ù„Ø¨ {len(news_items)} Ø®Ø¨Ø± Ù…Ù† {feed_name}")
        return news_items
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ {feed_name}: {e}")
        return []


def clean_text(text: str) -> str:
    """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ Ù…Ù† HTML ÙˆØ§Ù„Ø£Ø­Ø±Ù Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©"""
    if not text:
        return ""
    
    # Ø¥Ø²Ø§Ù„Ø© HTML tags
    text = re.sub(r'<[^>]+>', '', text)
    
    # Ø¥Ø²Ø§Ù„Ø© Ù…Ø³Ø§ÙØ§Øª Ø²Ø§Ø¦Ø¯Ø© ÙˆÙ…Ø­Ø§Ø±Ù Ø®Ø§ØµØ©
    text = re.sub(r'\s+', ' ', text)
    text = text.strip()
    
    return text


def shorten_url(url: str) -> str:
    """Ø§Ø®ØªØµØ§Ø± Ø§Ù„Ø±Ø§Ø¨Ø· Ù„Ø¹Ø±Ø¶ Ø£ÙØ¶Ù„"""
    try:
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø³Ù… Ø§Ù„Ù…ÙˆÙ‚Ø¹ ÙÙ‚Ø·
        from urllib.parse import urlparse
        parsed = urlparse(url)
        domain = parsed.netloc.replace('www.', '')
        return domain
    except:
        return url[:30] + '...'


def are_similar_news(title1: str, title2: str) -> bool:
    """
    Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù„Ù…Ù†Ø¹ Ø§Ù„ØªÙƒØ±Ø§Ø±
    ÙŠÙ‚Ø§Ø±Ù† Ø£ÙˆÙ„ 50 Ø­Ø±Ù Ù…Ù† Ø§Ù„Ø¹Ù†ÙˆØ§Ù†ÙŠÙ†
    """
    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ†
    t1 = clean_text(title1).lower()[:50]
    t2 = clean_text(title2).lower()[:50]
    
    # Ø­Ø³Ø§Ø¨ Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ´Ø§Ø¨Ù‡
    if len(t1) < 10 or len(t2) < 10:
        return False
    
    # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø£Ø­Ø¯Ù‡Ù…Ø§ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø¢Ø®Ø±
    if t1 in t2 or t2 in t1:
        return True
    
    # Ø­Ø³Ø§Ø¨ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø´ØªØ±ÙƒØ©
    words1 = set(t1.split())
    words2 = set(t2.split())
    common = words1.intersection(words2)
    
    # Ø¥Ø°Ø§ ÙƒØ§Ù† 70% Ù…Ù† Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ù…Ø´ØªØ±ÙƒØ©
    similarity = len(common) / max(len(words1), len(words2))
    return similarity > 0.7


def format_news_message(news_item: Dict) -> str:
    """ØªÙ†Ø³ÙŠÙ‚ Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø®Ø¨Ø± Ø¨Ø´ÙƒÙ„ Ø§Ø­ØªØ±Ø§ÙÙŠ"""
    title = clean_text(news_item['title'])
    link = news_item['link']
    source = news_item['source']
    summary = clean_text(news_item.get('summary', ''))
    published = news_item.get('published', '')
    
    # ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ø®Ø¨Ø± (Ø£ÙŠÙ‚ÙˆÙ†Ø© Ø­Ø³Ø¨ Ø§Ù„Ù…Ø­ØªÙˆÙ‰)
    icon = "ğŸ“°"
    title_lower = title.lower()
    summary_lower = summary.lower()
    full_text = f"{title_lower} {summary_lower}"
    
    if any(word in full_text for word in ['ÙˆØ¸ÙŠÙØ©', 'ÙˆØ¸Ø§Ø¦Ù', 'ØªÙˆØ¸ÙŠÙ', 'ØªØ¹ÙŠÙŠÙ†']):
        icon = "ğŸ’¼"
    elif any(word in full_text for word in ['Ù…Ø´Ø±ÙˆØ¹', 'Ù…Ø´Ø§Ø±ÙŠØ¹', 'Ø¨Ù†Ø§Ø¡', 'Ø¥Ù†Ø´Ø§Ø¡', 'ØªØ·ÙˆÙŠØ±']):
        icon = "ğŸ—ï¸"
    elif any(word in full_text for word in ['ØªØ±Ø³ÙŠØ©', 'ØªØ±Ø³ÙŠØ§Øª', 'Ù…Ù†Ø§Ù‚ØµØ©', 'Ø¹Ù‚Ø¯']):
        icon = "ğŸ“‹"
    elif any(word in full_text for word in ['Ø§Ø³ØªØ«Ù…Ø§Ø±', 'Ø§Ø³ØªØ«Ù…Ø§Ø±Ø§Øª', 'Ù…Ù„ÙŠØ§Ø±', 'Ù…Ù„ÙŠÙˆÙ†']):
        icon = "ğŸ’°"
    elif any(word in full_text for word in ['Ù…Ø¯Ø±Ø³Ø©', 'Ø¬Ø§Ù…Ø¹Ø©', 'ØªØ¹Ù„ÙŠÙ…', 'Ø¯Ø±Ø§Ø³ÙŠ']):
        icon = "ğŸ“"
    elif any(word in full_text for word in ['Ù…Ø³ØªØ´ÙÙ‰', 'ØµØ­Ø©', 'Ø·Ø¨ÙŠ', 'Ø¹Ù„Ø§Ø¬']):
        icon = "ğŸ¥"
    elif any(word in full_text for word in ['Ø·Ù‚Ø³', 'Ø£Ù…Ø·Ø§Ø±', 'Ø­Ø±Ø§Ø±Ø©', 'Ø¶Ø¨Ø§Ø¨', 'Ø£Ø±ØµØ§Ø¯']):
        icon = "ğŸŒ¤ï¸"
    
    # ØªÙ‚Ù„ÙŠØµ Ø§Ù„Ù…Ù„Ø®Øµ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø·ÙˆÙŠÙ„Ø§Ù‹
    if summary and len(summary) > 180:
        summary = summary[:177] + '...'
    
    # Ø±Ø³Ø§Ù„Ø© Ù…Ø®ØªØµØ±Ø©: ÙÙ‚Ø· Ø§Ù„Ø¹Ù†ÙˆØ§Ù† + Ø§Ù„ØªØ§Ø±ÙŠØ® + Ø§Ù„Ù…ØµØ¯Ø± (Ø¨Ø¯ÙˆÙ† Ø±ÙˆØ§Ø¨Ø·)
    message = f"{icon} {title}\n\n"
    
    # Ø¥Ø¶Ø§ÙØ© Ø§Ù„ØªØ§Ø±ÙŠØ®
    time_info = ""
    if published:
        try:
            news_date = date_parser.parse(published)
            time_ago = get_time_ago(news_date)
            time_info = f"ğŸ• {time_ago}"
        except:
            pass
    
    # Ø³Ø·Ø± ÙˆØ§Ø­Ø¯: Ø§Ù„ØªØ§Ø±ÙŠØ® + Ø§Ù„Ù…ØµØ¯Ø±
    if time_info:
        message += f"{time_info} â€¢ ğŸ“Œ {source}"
    else:
        message += f"ğŸ“Œ {source}"
    
    return message


def get_time_ago(news_date: datetime) -> str:
    """Ø­Ø³Ø§Ø¨ Ø§Ù„ÙØ±Ù‚ Ø§Ù„Ø²Ù…Ù†ÙŠ Ø¨Ø´ÙƒÙ„ Ù…ÙÙ‡ÙˆÙ… Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"""
    if news_date.tzinfo:
        news_date = news_date.replace(tzinfo=None)
    
    now = datetime.now()
    diff = now - news_date
    
    if diff.days > 0:
        if diff.days == 1:
            return "Ù…Ù†Ø° ÙŠÙˆÙ… ÙˆØ§Ø­Ø¯"
        elif diff.days == 2:
            return "Ù…Ù†Ø° ÙŠÙˆÙ…ÙŠÙ†"
        else:
            return f"Ù…Ù†Ø° {diff.days} Ø£ÙŠØ§Ù…"
    
    hours = diff.seconds // 3600
    if hours > 0:
        if hours == 1:
            return "Ù…Ù†Ø° Ø³Ø§Ø¹Ø© ÙˆØ§Ø­Ø¯Ø©"
        elif hours == 2:
            return "Ù…Ù†Ø° Ø³Ø§Ø¹ØªÙŠÙ†"
        else:
            return f"Ù…Ù†Ø° {hours} Ø³Ø§Ø¹Ø§Øª"
    
    minutes = diff.seconds // 60
    if minutes > 0:
        if minutes == 1:
            return "Ù…Ù†Ø° Ø¯Ù‚ÙŠÙ‚Ø© ÙˆØ§Ø­Ø¯Ø©"
        elif minutes == 2:
            return "Ù…Ù†Ø° Ø¯Ù‚ÙŠÙ‚ØªÙŠÙ†"
        else:
            return f"Ù…Ù†Ø° {minutes} Ø¯Ù‚ÙŠÙ‚Ø©"
    
    return "Ù…Ù†Ø° Ù„Ø­Ø¸Ø§Øª"


def send_telegram_message(chat_id: int, message: str, retry_count: int = 3) -> bool:
    """Ø¥Ø±Ø³Ø§Ù„ Ø±Ø³Ø§Ù„Ø© Ù„Ù…Ø¬Ù…ÙˆØ¹Ø©/Ù‚Ù†Ø§Ø© ÙÙŠ ØªÙ„ÙŠØ¬Ø±Ø§Ù… Ù…Ø¹ retry"""
    for attempt in range(retry_count):
        try:
            payload = {
                'chat_id': chat_id,
                'text': message,
                'disable_web_page_preview': False
            }
            
            response = requests.post(
                f'{TELEGRAM_API}/sendMessage',
                json=payload,
                timeout=10
            )
            
            result = response.json()
            if result.get('ok'):
                return True
            else:
                error_desc = result.get('description', '')
                
                # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø®Ø·Ø£ "Too Many Requests"ØŒ Ø§Ù†ØªØ¸Ø± ÙˆØ£Ø¹Ø¯ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø©
                if 'Too Many Requests' in error_desc:
                    retry_after = result.get('parameters', {}).get('retry_after', 5)
                    print(f"â³ ØªÙ„ÙŠØ¬Ø±Ø§Ù… ÙŠØ·Ù„Ø¨ Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± {retry_after} Ø«Ø§Ù†ÙŠØ©...")
                    time.sleep(retry_after + 1)
                    continue
                else:
                    print(f"âŒ ÙØ´Ù„ Ø§Ù„Ø¥Ø±Ø³Ø§Ù„ Ù„Ù€ {chat_id}: {error_desc}")
                    return False
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¥Ø±Ø³Ø§Ù„ (Ù…Ø­Ø§ÙˆÙ„Ø© {attempt + 1}): {e}")
            if attempt < retry_count - 1:
                time.sleep(2)
                continue
            return False
    
    return False


def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© - ÙØµÙ„ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø¥Ù„Ù‰: Ø·Ù‚Ø³ØŒ ÙˆØ¸Ø§Ø¦ÙØŒ Ø¹Ø§Ù…Ø©"""
    print(f"\nğŸ¤– Ø¨Ø¯Ø¡ Ø¨ÙˆØª Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø´Ø±Ù‚ÙŠØ© - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ù…Ø±Ø³Ù„Ø© Ø³Ø§Ø¨Ù‚Ø§Ù‹
    sent_news = load_sent_news()
    
    # Ø¬Ù„Ø¨ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª
    chat_ids = get_bot_chats()
    
    # Ø¥Ø¶Ø§ÙØ© chat IDs ÙŠØ¯ÙˆÙŠØ§Ù‹ Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„ÙŠÙ‡Ø§ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹
    if not chat_ids:
        chat_ids = [-1003882183490]  # Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
    
    if not chat_ids:
        print("âš ï¸  Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª. ØªØ£ÙƒØ¯ Ù…Ù†:")
        print("   1. Ø§Ù„Ø¨ÙˆØª Ù…Ø¶Ø§Ù Ù„Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª")
        print("   2. Ø§Ù„Ø¨ÙˆØª Ù„Ø¯ÙŠÙ‡ ØµÙ„Ø§Ø­ÙŠØ§Øª Ø§Ù„Ø¥Ø±Ø³Ø§Ù„")
        print("   3. Ù‡Ù†Ø§Ùƒ Ø±Ø³Ø§Ø¦Ù„ Ø³Ø§Ø¨Ù‚Ø© ÙÙŠ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª")
        print("\nğŸ’¡ ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø¶Ø§ÙØ© chat IDs ÙŠØ¯ÙˆÙŠØ§Ù‹ ÙÙŠ Ø§Ù„ÙƒÙˆØ¯")
        return
    
    print(f"ğŸ“± ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(chat_ids)} Ù…Ø¬Ù…ÙˆØ¹Ø©/Ù‚Ù†Ø§Ø©")
    
    # 1ï¸âƒ£ Ø¬Ù„Ø¨ Ø£Ø®Ø¨Ø§Ø± Ø§Ù„ÙˆØ¸Ø§Ø¦Ù (Ù…Ù†ÙØµÙ„Ø©)
    print("\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print("ğŸ’¼ Ø¬Ù„Ø¨ Ø£Ø®Ø¨Ø§Ø± Ø§Ù„ÙˆØ¸Ø§Ø¦Ù...")
    jobs_news = []
    for feed in JOBS_NEWS_FEEDS:
        if feed.get('enabled', True):
            news_items = fetch_rss_news(feed['url'], feed['name'])
            jobs_news.extend(news_items)
    
    # ÙÙ„ØªØ±Ø© Ø£Ø®Ø¨Ø§Ø± Ø§Ù„ÙˆØ¸Ø§Ø¦Ù
    jobs_eastern = [n for n in jobs_news if is_eastern_province_news(n) and is_jobs_news(n) and is_recent_news(n, max_days=2)]
    jobs_unique = remove_duplicates(jobs_eastern)
    print(f"ğŸ’¼ Ø£Ø®Ø¨Ø§Ø± ÙˆØ¸Ø§Ø¦Ù Ø­Ø¯ÙŠØ«Ø©: {len(jobs_unique)}")
    
    # 2ï¸âƒ£ Ø¬Ù„Ø¨ Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø·Ù‚Ø³ (Ù…Ù†ÙØµÙ„Ø©)
    print("\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print("ğŸŒ¤ï¸  Ø¬Ù„Ø¨ Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø·Ù‚Ø³...")
    weather_news = []
    for feed in WEATHER_NEWS_FEEDS:
        if feed.get('enabled', True):
            news_items = fetch_rss_news(feed['url'], feed['name'])
            weather_news.extend(news_items)
    
    # ÙÙ„ØªØ±Ø© Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø·Ù‚Ø³
    weather_eastern = [n for n in weather_news if is_eastern_province_news(n) and is_weather_news(n) and is_recent_news(n, max_days=1)]
    weather_unique = remove_duplicates(weather_eastern)
    print(f"ğŸŒ¤ï¸  Ø£Ø®Ø¨Ø§Ø± Ø·Ù‚Ø³ Ø­Ø¯ÙŠØ«Ø©: {len(weather_unique)}")
    
    # 3ï¸âƒ£ Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø¹Ø§Ù…Ø© (Ù…Ø´Ø§Ø±ÙŠØ¹ØŒ ØªØ±Ø³ÙŠØ§ØªØŒ Ø§Ø³ØªØ«Ù…Ø§Ø±Ø§Øª...)
    print("\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print("ğŸ“° Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø¹Ø§Ù…Ø©...")
    general_news = []
    for feed in GENERAL_NEWS_FEEDS:
        if feed.get('enabled', True):
            news_items = fetch_rss_news(feed['url'], feed['name'])
            general_news.extend(news_items)
    
    # ÙÙ„ØªØ±Ø© Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø¹Ø§Ù…Ø© (Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ Ø§Ù„Ø¨Ø±ÙˆØªÙˆÙƒÙˆÙ„ÙŠØ©)
    general_eastern = []
    protocol_count = 0
    for news in general_news:
        if not is_eastern_province_news(news):
            continue
        if is_protocol_news(news):
            protocol_count += 1
            continue
        if not is_recent_news(news, max_days=2):
            continue
        # Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ Ø£Ø®Ø¨Ø§Ø± Ø§Ù„ÙˆØ¸Ø§Ø¦Ù ÙˆØ§Ù„Ø·Ù‚Ø³ (Ù„Ù‡Ø§ Ù‚Ø³Ù… Ø®Ø§Øµ)
        if is_jobs_news(news) or is_weather_news(news):
            continue
        if is_valuable_news(news):
            general_eastern.append(news)
    
    general_unique = remove_duplicates(general_eastern)
    print(f"ğŸš« ØªÙ… Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ {protocol_count} Ø®Ø¨Ø± Ø¨Ø±ÙˆØªÙˆÙƒÙˆÙ„ÙŠ")
    print(f"ğŸ“° Ø£Ø®Ø¨Ø§Ø± Ø¹Ø§Ù…Ø© Ø­Ø¯ÙŠØ«Ø©: {len(general_unique)}")
    
    # 4ï¸âƒ£ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø¨Ø´ÙƒÙ„ Ù…Ù†ÙØµÙ„
    print("\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print("ğŸ“¤ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±...")
    
    total_sent = 0
    
    # Ø¥Ø±Ø³Ø§Ù„ Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø·Ù‚Ø³ (Ø±Ø³Ø§Ù„Ø© Ø¬Ù…Ø§Ø¹ÙŠØ© ÙˆØ§Ø­Ø¯Ø©)
    if weather_unique:
        weather_new = filter_new_news(weather_unique, sent_news)
        if weather_new:
            weather_message = "ğŸŒ¤ï¸ *Ø·Ù‚Ø³ Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø´Ø±Ù‚ÙŠØ©*\n" + "â”" * 30 + "\n\n"
            for news in weather_new[:3]:  # Ø£Ù‚ØµÙ‰ 3 Ø£Ø®Ø¨Ø§Ø± Ø·Ù‚Ø³
                weather_message += f"â€¢ {news['title']}\n"
                weather_message += f"  ğŸ“Œ {news['source']}\n\n"
                mark_as_sent(news, sent_news)
            
            for chat_id in chat_ids:
                if send_telegram_message(chat_id, weather_message):
                    total_sent += 1
                    print(f"âœ… ØªÙ… Ø¥Ø±Ø³Ø§Ù„ Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø·Ù‚Ø³ ({len(weather_new)} Ø£Ø®Ø¨Ø§Ø±)")
            time.sleep(2)
    
    # Ø¥Ø±Ø³Ø§Ù„ Ø£Ø®Ø¨Ø§Ø± Ø§Ù„ÙˆØ¸Ø§Ø¦Ù (Ø±Ø³Ø§Ù„Ø© Ø¬Ù…Ø§Ø¹ÙŠØ© ÙˆØ§Ø­Ø¯Ø©)
    if jobs_unique:
        jobs_new = filter_new_news(jobs_unique, sent_news)
        if jobs_new:
            jobs_message = "ğŸ’¼ *ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø´Ø±Ù‚ÙŠØ©*\n" + "â”" * 30 + "\n\n"
            for news in jobs_new[:5]:  # Ø£Ù‚ØµÙ‰ 5 ÙˆØ¸Ø§Ø¦Ù
                jobs_message += f"â€¢ {news['title']}\n"
                jobs_message += f"  ğŸ“Œ {news['source']}\n\n"
                mark_as_sent(news, sent_news)
            
            for chat_id in chat_ids:
                if send_telegram_message(chat_id, jobs_message):
                    total_sent += 1
                    print(f"âœ… ØªÙ… Ø¥Ø±Ø³Ø§Ù„ Ø£Ø®Ø¨Ø§Ø± Ø§Ù„ÙˆØ¸Ø§Ø¦Ù ({len(jobs_new)} ÙˆØ¸Ø§Ø¦Ù)")
            time.sleep(2)
    
    # Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø¹Ø§Ù…Ø© (Ø±Ø³Ø§Ø¦Ù„ Ù…Ù†ÙØµÙ„Ø© Ù…Ø®ØªØµØ±Ø©)
    if general_unique:
        general_new = filter_new_news(general_unique, sent_news)
        if general_new:
            for i, news in enumerate(general_new[:6], 1):  # Ø£Ù‚ØµÙ‰ 6 Ø£Ø®Ø¨Ø§Ø± Ø¹Ø§Ù…Ø©
                message = format_news_message(news)
                for chat_id in chat_ids:
                    if send_telegram_message(chat_id, message):
                        total_sent += 1
                        print(f"âœ… [{i}/{min(len(general_new), 6)}] Ø£Ø®Ø¨Ø§Ø± Ø¹Ø§Ù…Ø©: {news['title'][:50]}...")
                    time.sleep(1)
                mark_as_sent(news, sent_news)
    
    # Ø­ÙØ¸ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ù…Ø±Ø³Ù„Ø©
    save_sent_news(sent_news)
    
    print(f"\nâœ¨ ØªÙ… Ø¥Ø±Ø³Ø§Ù„ {total_sent} Ø±Ø³Ø§Ù„Ø© Ø¨Ù†Ø¬Ø§Ø­!")
    print("=" * 60)


def remove_duplicates(news_list: List[Dict]) -> List[Dict]:
    """Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ù…ÙƒØ±Ø±Ø©"""
    unique = []
    for news in news_list:
        is_duplicate = False
        for existing in unique:
            if are_similar_news(news['title'], existing['title']):
                is_duplicate = True
                break
        if not is_duplicate:
            unique.append(news)
    return unique


def filter_new_news(news_list: List[Dict], sent_news: Dict) -> List[Dict]:
    """ÙÙ„ØªØ±Ø© Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© ÙÙ‚Ø·"""
    new_news = []
    for news in news_list:
        if news['id'] not in sent_news:
            new_news.append(news)
    return new_news


def mark_as_sent(news: Dict, sent_news: Dict):
    """ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø®Ø¨Ø± ÙƒÙ…ÙØ±Ø³Ù„"""
    sent_news[news['id']] = {
        'title': news['title'],
        'sent_at': datetime.now().isoformat()
    }


if __name__ == '__main__':
    main()
